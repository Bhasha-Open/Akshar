{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f30f47f",
   "metadata": {},
   "source": [
    "# Tokenizer known as akshar\n",
    "\n",
    "This notebook demonstrates **every feature** of akshar, the linguistically-aware tokenizer for Hindi, Sanskrit, and Hinglish.\n",
    "\n",
    "## What You'll See:\n",
    "- akshar segmentation (conjunct preservation)\n",
    "- Code-switch detection\n",
    "- Text normalization\n",
    "- Hinglish processing\n",
    "- Sanskrit handling\n",
    "- And much more...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce972786",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import the tokenizer and helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4c275f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from akshar import aksharTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356f0891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e97da3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = aksharTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f7a26564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'a', 'j', ' ', '‡§Æ‡•å', '‡§∏', '‡§Æ', ' ', '‡§¨', '‡§π‡•Å', '‡§§', ' ', '‡§Ö', '‡§ö‡•ç‡§õ‡§æ', ' ', '‡§π‡•à']\n"
     ]
    }
   ],
   "source": [
    "text = \"aaj ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "word_level_heading",
   "metadata": {},
   "source": [
    "## Word-Level Tokenization\n",
    "\n",
    "Get word-level tokens (like IndicNLP) using akshar's preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9103869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what we have \n",
    "\n",
    "from indicnlp import common\n",
    "common.set_resources_path(\"indic_nlp_resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e61e2245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‡§®‡§æ‡§∏‡•ë‡§¶‡§æ‡§∏‡•Ä‡•í‡§®‡•ç‡§®‡•ã', '‡§∏‡§¶‡§æ‡•ë‡§∏‡•Ä‡§§‡•ç‡§§‡•í‡§¶‡§æ‡§®‡•Ä‡§Ç‡•í', '‡§®‡§æ‡§∏‡•Ä‡•í‡§¶‡•ç‡§∞‡§ú‡•ã‡•í', '‡§®‡•ã', '‡§µ‡§Ø‡•ã‡•ë‡§Æ‡§æ', '‡§™‡•í‡§∞‡•ã', '‡§Ø‡§§‡•ç', '‡•§', '‡•§']\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.tokenize.indic_tokenize import trivial_tokenize\n",
    "\n",
    "sentence = \"‡§®‡§æ‡§∏‡•ë‡§¶‡§æ‡§∏‡•Ä‡•í‡§®‡•ç‡§®‡•ã ‡§∏‡§¶‡§æ‡•ë‡§∏‡•Ä‡§§‡•ç‡§§‡•í‡§¶‡§æ‡§®‡•Ä‡§Ç‡•í ‡§®‡§æ‡§∏‡•Ä‡•í‡§¶‡•ç‡§∞‡§ú‡•ã‡•í ‡§®‡•ã ‡§µ‡§Ø‡•ã‡•ë‡§Æ‡§æ ‡§™‡•í‡§∞‡•ã ‡§Ø‡§§‡•ç‡•§‡•§\"\n",
    "print(trivial_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "word_level_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what we offer \n",
    "\n",
    "from akshar import word_tokenize_hindi,word_tokenize,word_tokenize_sanskrit\n",
    "\n",
    "# test it\n",
    "text_hi = \"aaj ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\"\n",
    "text_sa = \"‡§®‡§æ‡§∏‡•ë‡§¶‡§æ‡§∏‡•Ä‡•í‡§®‡•ç‡§®‡•ã ‡§∏‡§¶‡§æ‡•ë‡§∏‡•Ä‡§§‡•ç‡§§‡•í‡§¶‡§æ‡§®‡•Ä‡§Ç‡•í ‡§®‡§æ‡§∏‡•Ä‡•í‡§¶‡•ç‡§∞‡§ú‡•ã‡•í ‡§®‡•ã ‡§µ‡§Ø‡•ã‡•ë‡§Æ‡§æ ‡§™‡•í‡§∞‡•ã ‡§Ø‡§§‡•ç‡•§\"\n",
    "text_idk = \"‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ what ‡§∏‡§¶‡§æ‡•ë‡§∏‡•Ä‡§§‡•ç‡§§‡•í‡§¶‡§æ‡§®‡•Ä‡§Ç‡•í\"\n",
    "text = text_hi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfc10e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi : ['aaj', '‡§Æ‡•å‡§∏‡§Æ', '‡§¨‡§π‡•Å‡§§', '‡§Ö‡§ö‡•ç‡§õ‡§æ', '‡§π‡•à'] \n",
      " Sanskrit : ['‡§®‡§æ‡§∏‡•ë‡§¶‡§æ‡§∏‡•Ä‡•í‡§®‡•ç‡§®‡•ã', '‡§∏‡§¶‡§æ‡•ë‡§∏‡•Ä‡§§‡•ç‡§§‡•í‡§¶‡§æ‡§®‡•Ä‡§Ç‡•í', '‡§®‡§æ‡§∏‡•Ä‡•í‡§¶‡•ç‡§∞‡§ú‡•ã‡•í', '‡§®‡•ã', '‡§µ‡§Ø‡•ã‡•ë‡§Æ‡§æ', '‡§™‡•í‡§∞‡•ã', '‡§Ø‡§§‡•ç', '‡•§'] \n",
      " mix : ['‡§Æ‡•å‡§∏‡§Æ', '‡§¨‡§π‡•Å‡§§', 'what', '‡§∏‡§¶‡§æ‡•ë‡§∏‡•Ä‡§§‡•ç‡§§‡•í‡§¶‡§æ‡§®‡•Ä‡§Ç‡•í'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hi = word_tokenize_hindi(text_hi)\n",
    "sa = word_tokenize_hindi(text_sa)\n",
    "ikik = word_tokenize_hindi(text_idk)\n",
    "\n",
    "print(f\"Hindi : {hi} \\n Sanskrit : {sa} \\n mix : {ikik} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dbc6442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'original': 'aaj ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à', 'normalized': 'aaj ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à', 'akshars': ['a', 'a', 'j', ' ', '‡§Æ‡•å', '‡§∏', '‡§Æ', ' ', '‡§¨', '‡§π‡•Å', '‡§§', ' ', '‡§Ö', '‡§ö‡•ç‡§õ‡§æ', ' ', '‡§π‡•à'], 'code_switches': [('aaj ', 'roman'), ('‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à', 'devanagari')], 'tokens': ['a', 'a', 'j', ' ', '‡§Æ‡•å', '‡§∏', '‡§Æ', ' ', '‡§¨', '‡§π‡•Å', '‡§§', ' ', '‡§Ö', '‡§ö‡•ç‡§õ‡§æ', ' ', '‡§π‡•à'], 'stats': {'akshar_count': 16, 'script_switches': 1, 'devanagari_ratio': 0.8181818181818182, 'roman_ratio': 0.18181818181818182}}\n"
     ]
    }
   ],
   "source": [
    "analysis = tokenizer.explain(text)\n",
    "print(analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9aa49b",
   "metadata": {},
   "source": [
    "# akshar Features Demo\n",
    "\n",
    "This notebook demonstrates all the intelligent features of akshar tokenizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1206b8a",
   "metadata": {},
   "source": [
    "## 1. Basic Tokenization\n",
    "\n",
    "Tokenize text without any trained model (falls back to akshar-level)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d7336831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\n",
      "Tokens (15): ‡§Ü | ‡§ú |   | ‡§Æ‡•å | ‡§∏ | ‡§Æ |   | ‡§¨ | ‡§π‡•Å | ‡§§ |   | ‡§Ö | ‡§ö‡•ç‡§õ‡§æ |   | ‡§π‡•à\n",
      "\n",
      "Text: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ\n",
      "Tokens (7): ‡§® | ‡§Æ | ‡§∏‡•ç‡§§‡•á |   | ‡§¶‡•Å | ‡§®‡§ø | ‡§Ø‡§æ\n",
      "\n",
      "Text: ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á ‡§ß‡§∞‡•ç‡§Æ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á\n",
      "Tokens (7): ‡§ï‡•ç‡§∑‡•á | ‡§§‡•ç‡§∞‡•á |   | ‡§ß | ‡§∞‡•ç‡§Æ | ‡§ï‡•ç‡§∑‡•á | ‡§§‡•ç‡§∞‡•á\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\",\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ\",\n",
    "    \"‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á ‡§ß‡§∞‡•ç‡§Æ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á\",\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Tokens ({len(tokens)}): {' | '.join(tokens)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987db74",
   "metadata": {},
   "source": [
    "## 2. akshar Segmentation\n",
    "\n",
    "Shows how Devanagari conjuncts stay together as single units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41f7f0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞\n",
      "akshars: [ ‡§ï‡•ç‡§∑‡•á ] [ ‡§§‡•ç‡§∞ ]\n",
      "Count: 2\n",
      "\n",
      "Text: ‡§ú‡•ç‡§û‡§æ‡§®\n",
      "akshars: [ ‡§ú‡•ç‡§û‡§æ ] [ ‡§® ]\n",
      "Count: 2\n",
      "\n",
      "Text: ‡§§‡•ç‡§∞‡§ø‡§∂‡•Ç‡§≤\n",
      "akshars: [ ‡§§‡•ç‡§∞‡§ø ] [ ‡§∂‡•Ç ] [ ‡§≤ ]\n",
      "Count: 3\n",
      "\n",
      "Text: ‡§ß‡§∞‡•ç‡§Æ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á\n",
      "akshars: [ ‡§ß ] [ ‡§∞‡•ç‡§Æ ] [ ‡§ï‡•ç‡§∑‡•á ] [ ‡§§‡•ç‡§∞‡•á ]\n",
      "Count: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from akshar import segment_akshars\n",
    "\n",
    "sanskrit_examples = [\n",
    "    \"‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞\",      # kShetra - notice ‡§ï‡•ç‡§∑ stays together\n",
    "    \"‡§ú‡•ç‡§û‡§æ‡§®\",        # gyaan - ‡§ú‡•ç‡§û stays together\n",
    "    \"‡§§‡•ç‡§∞‡§ø‡§∂‡•Ç‡§≤\",      # trishul - ‡§§‡•ç‡§∞ stays together\n",
    "    \"‡§ß‡§∞‡•ç‡§Æ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á\",  # dharmakshetre - multiple conjuncts\n",
    "]\n",
    "\n",
    "for text in sanskrit_examples:\n",
    "    akshars = segment_akshars(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"akshars: [ {' ] [ '.join(akshars)} ]\")\n",
    "    print(f\"Count: {len(akshars)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f4597",
   "metadata": {},
   "source": [
    "## 3. Code-Switch Detection\n",
    "\n",
    "Detects boundaries where script changes (Hinglish!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f829f61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: aaj ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\n",
      "Script Boundaries:\n",
      "  [roman       ] 'aaj '\n",
      "  [devanagari  ] '‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à'\n",
      "\n",
      "Text: ‡§Æ‡•à‡§Ç California ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§§‡§æ ‡§π‡•Ç‡§Ç\n",
      "Script Boundaries:\n",
      "  [devanagari  ] '‡§Æ‡•à‡§Ç '\n",
      "  [roman       ] 'California '\n",
      "  [devanagari  ] '‡§Æ‡•á‡§Ç ‡§∞‡§π‡§§‡§æ ‡§π‡•Ç‡§Ç'\n",
      "\n",
      "Text: hello ‡§Ø‡§æ‡§∞ kya ‡§π‡§æ‡§≤ ‡§π‡•à\n",
      "Script Boundaries:\n",
      "  [roman       ] 'hello '\n",
      "  [devanagari  ] '‡§Ø‡§æ‡§∞ '\n",
      "  [roman       ] 'kya '\n",
      "  [devanagari  ] '‡§π‡§æ‡§≤ ‡§π‡•à'\n",
      "\n",
      "Text: ‡§Ü‡§ú ‡§ï‡§æ day ‡§¨‡§π‡•Å‡§§ nice ‡§•‡§æ\n",
      "Script Boundaries:\n",
      "  [devanagari  ] '‡§Ü‡§ú ‡§ï‡§æ '\n",
      "  [roman       ] 'day '\n",
      "  [devanagari  ] '‡§¨‡§π‡•Å‡§§ '\n",
      "  [roman       ] 'nice '\n",
      "  [devanagari  ] '‡§•‡§æ'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from akshar import detect_code_switches\n",
    "\n",
    "hinglish_examples = [\n",
    "    \"aaj ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\",\n",
    "    \"‡§Æ‡•à‡§Ç California ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§§‡§æ ‡§π‡•Ç‡§Ç\",\n",
    "    \"hello ‡§Ø‡§æ‡§∞ kya ‡§π‡§æ‡§≤ ‡§π‡•à\",\n",
    "    \"‡§Ü‡§ú ‡§ï‡§æ day ‡§¨‡§π‡•Å‡§§ nice ‡§•‡§æ\",\n",
    "]\n",
    "\n",
    "for text in hinglish_examples:\n",
    "    switches = detect_code_switches(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Script Boundaries:\")\n",
    "    for segment, script in switches:\n",
    "        print(f\"  [{script:12}] '{segment}'\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78085729",
   "metadata": {},
   "source": [
    "## 4. Text Normalization\n",
    "\n",
    "Handles Unicode, semantic normalization, and Hinglish cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fec15c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Normalization (lowercase Roman, preserve Devanagari):\n",
      "\n",
      "Original:   Hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á WORLD\n",
      "Normalized: hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á world\n",
      "\n",
      "Original:   Heyyy ‡§Ø‡§æ‡§∞ kya HAAL hai\n",
      "Normalized: hey ‡§Ø‡§æ‡§∞ kya haal hai\n",
      "\n",
      "Original:   ‡§Æ‡•à‡§Ç California ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§§‡§æ ‡§π‡•Ç‡§Ç\n",
      "Normalized: ‡§Æ‡•à‡§Ç california ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§§‡§æ ‡§π‡•Ç‡§Ç\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from akshar import normalize_text\n",
    "\n",
    "normalization_examples = [\n",
    "    \"Hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á WORLD\",           # mixed case Roman\n",
    "    \"Heyyy ‡§Ø‡§æ‡§∞ kya HAAL hai\",       # elongations + case\n",
    "    \"‡§Æ‡•à‡§Ç California ‡§Æ‡•á‡§Ç ‡§∞‡§π‡§§‡§æ ‡§π‡•Ç‡§Ç\",  # mixed scripts\n",
    "]\n",
    "\n",
    "print(\"Text Normalization (lowercase Roman, preserve Devanagari):\\n\")\n",
    "for text in normalization_examples:\n",
    "    normalized = normalize_text(text)\n",
    "    print(f\"Original:   {text}\")\n",
    "    print(f\"Normalized: {normalized}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3371af",
   "metadata": {},
   "source": [
    "## 5. Hinglish Normalization\n",
    "\n",
    "Handles elongations and variations common in social media\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "56286077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elongation Removal:\n",
      "\n",
      "heyyy           ‚Üí hey\n",
      "yaaaaar         ‚Üí yar\n",
      "niceeee         ‚Üí nice\n",
      "bohoooot        ‚Üí bohot\n",
      "\n",
      "==================================================\n",
      "\n",
      "Phonetic Signatures (for alignment):\n",
      "\n",
      "Variations: nahi, nahii, nahee\n",
      "Signatures: nahi, nahii, nahi\n",
      "\n",
      "Variations: yaar, yar\n",
      "Signatures: yar, yar\n",
      "\n",
      "Variations: achha, acha, accha\n",
      "Signatures: acha, aca, acca\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from akshar.normalize import remove_elongations, roman_phonetic_signature\n",
    "\n",
    "hinglish_variations = [\n",
    "    \"heyyy\",\n",
    "    \"yaaaaar\", \n",
    "    \"niceeee\",\n",
    "    \"bohoooot\",\n",
    "]\n",
    "\n",
    "print(\"Elongation Removal:\\n\")\n",
    "for word in hinglish_variations:\n",
    "    cleaned = remove_elongations(word)\n",
    "    print(f\"{word:15} ‚Üí {cleaned}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nPhonetic Signatures (for alignment):\\n\")\n",
    "\n",
    "variations = [\n",
    "    [\"nahi\", \"nahii\", \"nahee\"],\n",
    "    [\"yaar\", \"yar\"],\n",
    "    [\"achha\", \"acha\", \"accha\"],\n",
    "]\n",
    "\n",
    "for group in variations:\n",
    "    signatures = [roman_phonetic_signature(w) for w in group]\n",
    "    print(f\"Variations: {', '.join(group)}\")\n",
    "    print(f\"Signatures: {', '.join(signatures)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2e5611",
   "metadata": {},
   "source": [
    "## 6. Detailed Analysis\n",
    "\n",
    "The `explain()` method shows complete breakdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3191fde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: yaar aaj ka ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ hai\n",
      "Normalized:    yaar aaj ka ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ hai\n",
      "\n",
      "akshars (26): y | a | a | r |   | a | a | j |   | k | a |   | ‡§Æ‡•å | ‡§∏ | ‡§Æ |   | ‡§¨ | ‡§π‡•Å | ‡§§ |  ...\n",
      "\n",
      "Tokens (26): y | a | a | r |   | a | a | j |   | k | a |   | ‡§Æ‡•å | ‡§∏ | ‡§Æ |   | ‡§¨ | ‡§π‡•Å | ‡§§ |  ...\n",
      "\n",
      "Code Switches:\n",
      "  [roman       ] 'yaar aaj ka '\n",
      "  [devanagari  ] '‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ '\n",
      "  [roman       ] 'hai'\n",
      "\n",
      "Statistics:\n",
      "  akshar count:    26\n",
      "  Script switches:  2\n",
      "  Devanagari ratio: 51.6%\n",
      "  Roman ratio:      48.4%\n"
     ]
    }
   ],
   "source": [
    "text = \"yaar aaj ka ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ hai\"\n",
    "analysis = tokenizer.explain(text)\n",
    "\n",
    "print(f\"Original Text: {analysis['original']}\")\n",
    "print(f\"Normalized:    {analysis['normalized']}\")\n",
    "print(f\"\\nakshars ({len(analysis['akshars'])}): {' | '.join(analysis['akshars'][:20])}...\")\n",
    "print(f\"\\nTokens ({len(analysis['tokens'])}): {' | '.join(analysis['tokens'][:20])}...\")\n",
    "\n",
    "print(f\"\\nCode Switches:\")\n",
    "for segment, script in analysis['code_switches']:\n",
    "    print(f\"  [{script:12}] '{segment}'\")\n",
    "\n",
    "print(f\"\\nStatistics:\")\n",
    "stats = analysis['stats']\n",
    "print(f\"  akshar count:    {stats['akshar_count']}\")\n",
    "print(f\"  Script switches:  {stats['script_switches']}\")\n",
    "print(f\"  Devanagari ratio: {stats['devanagari_ratio']:.1%}\")\n",
    "print(f\"  Roman ratio:      {stats['roman_ratio']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc99ecd5",
   "metadata": {},
   "source": [
    "### Feature 18: Morphological Segmentation\n",
    "\n",
    "Break words into morphemes (requires: pip install morfessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3155a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Hindi Morfessor model loaded!\n",
      "\n",
      "Morphological Segmentation:\n",
      "\n",
      "  ‡§ñ‡§æ‡§®‡§æ            ‚Üí ‡§ñ‡§æ‡§®‡§æ\n",
      "  ‡§™‡•Ä‡§®‡§æ            ‚Üí ‡§™‡•Ä + ‡§®‡§æ\n",
      "  ‡§¶‡•á‡§ñ‡§®‡§æ           ‚Üí ‡§¶‡•á‡§ñ + ‡§®‡§æ\n",
      "  ‡§ö‡§≤‡§®‡§æ            ‚Üí ‡§ö‡§≤ + ‡§®‡§æ\n",
      "  ‡§∏‡•ã‡§®‡§æ            ‚Üí ‡§∏‡•ã‡§®‡§æ\n",
      "  ‡§∏‡•Å‡§®‡§®‡§æ           ‚Üí ‡§∏‡•Å‡§® + ‡§®‡§æ\n",
      "  ‡§ñ‡•á‡§≤‡§®‡§æ           ‚Üí ‡§ñ‡•á‡§≤ + ‡§®‡§æ\n",
      "  ‡§≤‡§ø‡§ñ‡§®‡§æ           ‚Üí ‡§≤‡§ø‡§ñ + ‡§®‡§æ\n",
      "  ‡§™‡§¢‡§º‡§®‡§æ           ‚Üí ‡§™‡§¢‡§º + ‡§®‡§æ\n",
      "  ‡§∏‡•Ä‡§ñ‡§®‡§æ           ‚Üí ‡§∏‡•Ä‡§ñ + ‡§®‡§æ\n",
      "  ‡§ó‡§æ‡§®‡§æ            ‚Üí ‡§ó‡§æ + ‡§®‡§æ\n",
      "  ‡§π‡§Å‡§∏‡§®‡§æ           ‚Üí ‡§π‡§Å‡§∏ + ‡§®‡§æ\n",
      "  ‡§∞‡•ã‡§®‡§æ            ‚Üí ‡§∞‡•ã + ‡§®‡§æ\n",
      "  ‡§∏‡•ã‡§ö‡§®‡§æ           ‚Üí ‡§∏‡•ã‡§ö + ‡§®‡§æ\n",
      "  ‡§¨‡•ã‡§≤‡§®‡§æ           ‚Üí ‡§¨‡•ã‡§≤ + ‡§®‡§æ\n",
      "  ‡§™‡§¢‡§º‡§æ‡§à           ‚Üí ‡§™‡§¢‡§º‡§æ‡§à\n",
      "  ‡§≤‡§°‡§º‡§æ‡§à           ‚Üí ‡§≤‡§°‡§º‡§æ‡§à\n",
      "  ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä          ‚Üí ‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä\n",
      "  ‡§ú‡§≤‡•ç‡§¶‡•Ä           ‚Üí ‡§ú‡§≤‡•ç‡§¶ + ‡•Ä\n",
      "  ‡§≠‡•Ç‡§ñ             ‚Üí ‡§≠‡•Ç‡§ñ\n",
      "  ‡§≠‡§Ø              ‚Üí ‡§≠‡§Ø\n",
      "  ‡§ï‡§æ‡§¨‡•Ç            ‚Üí ‡§ï‡§æ‡§¨‡•Ç\n",
      "  ‡§ï‡§π‡§æ‡§®‡•Ä           ‚Üí ‡§ï‡§π‡§æ‡§®‡•Ä\n",
      "  ‡§ñ‡•Å‡§∂‡•Ä            ‚Üí ‡§ñ‡•Å‡§∂‡•Ä\n",
      "  ‡§â‡§¶‡§æ‡§∏‡•Ä           ‚Üí ‡§â‡§¶‡§æ‡§∏ + ‡•Ä\n",
      "  ‡§Æ‡§ú‡§¶‡•Ç‡§∞           ‚Üí ‡§Æ‡§ú‡§¶‡•Ç‡§∞\n",
      "  ‡§Æ‡§π‡§ø‡§≤‡§æ           ‚Üí ‡§Æ‡§π‡§ø‡§≤‡§æ\n",
      "  ‡§Ü‡§¶‡§Æ‡•Ä            ‚Üí ‡§Ü‡§¶‡§Æ‡•Ä\n",
      "  ‡§ï‡§¨‡•Ç‡§§‡§∞           ‚Üí ‡§ï‡§¨‡•Ç‡§§‡§∞\n",
      "  ‡§§‡§ø‡§§‡§≤‡•Ä           ‚Üí ‡§§‡§ø‡§§‡§≤‡•Ä\n",
      "  ‡§ö‡•Å‡§®‡•å‡§§‡•Ä          ‚Üí ‡§ö‡•Å‡§®‡•å‡§§‡•Ä\n",
      "  ‡§â‡§™‡§Ø‡•ã‡§ó           ‚Üí ‡§â‡§™‡§Ø‡•ã‡§ó\n",
      "  ‡§Ö‡§®‡•Å‡§≠‡§µ           ‚Üí ‡§Ö‡§®‡•Å‡§≠‡§µ\n",
      "  ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ        ‚Üí ‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ\n",
      "  ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï          ‚Üí ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï\n",
      "  ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§®       ‚Üí ‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§®\n",
      "  ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ        ‚Üí ‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ\n",
      "  ‡§Ö‡§ú‡•ç‡§û‡§æ‡§®‡§§‡§æ        ‚Üí ‡§Ö‡§ú‡•ç‡§û‡§æ‡§® + ‡§§‡§æ\n",
      "  ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ        ‚Üí ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ\n",
      "  ‡§™‡§∞‡§ø‡§≠‡§æ‡§∑‡§æ         ‚Üí ‡§™‡§∞‡§ø‡§≠‡§æ‡§∑‡§æ\n",
      "  ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø        ‚Üí ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø\n",
      "  ‡§µ‡§ø‡§ï‡§æ‡§∏           ‚Üí ‡§µ‡§ø‡§ï‡§æ‡§∏\n",
      "  ‡§∏‡§Æ‡§∞‡•ç‡§™‡§£          ‚Üí ‡§∏‡§Æ‡§∞‡•ç‡§™‡§£\n",
      "  ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏         ‚Üí ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏\n",
      "  ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ          ‚Üí ‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ\n",
      "  ‡§â‡§§‡•ç‡§§‡§∞‡§¶‡§æ‡§Ø‡§ø‡§§‡•ç‡§µ    ‚Üí ‡§â‡§§‡•ç‡§§‡§∞ + ‡§¶‡§æ‡§Ø‡§ø‡§§‡•ç‡§µ\n",
      "  ‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞‡§§‡§æ      ‚Üí ‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞‡§§‡§æ\n",
      "  ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§§‡§æ        ‚Üí ‡§®‡§æ‡§ó‡§∞‡§ø‡§ï + ‡§§‡§æ\n",
      "  ‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§ï         ‚Üí ‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§ï\n",
      "  ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§§‡§æ       ‚Üí ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞ + ‡§ø‡§§‡§æ\n",
      "  ‡§¶‡§æ‡§Ø‡§ø‡§§‡•ç‡§µ‡§π‡•Ä‡§®‡§§‡§æ    ‚Üí ‡§¶‡§æ‡§Ø‡§ø‡§§‡•ç‡§µ + ‡§π‡•Ä‡§®‡§§‡§æ\n",
      "  ‡§Ö‡§®‡•Å‡§∂‡§æ‡§∏‡§®‡§π‡•Ä‡§®‡§§‡§æ    ‚Üí ‡§Ö‡§®‡•Å‡§∂‡§æ‡§∏‡§® + ‡§π‡•Ä‡§®‡§§‡§æ\n",
      "  ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ‡§™‡§®      ‚Üí ‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ + ‡§™‡§®\n",
      "  ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§®        ‚Üí ‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§®\n",
      "  ‡§∏‡§Ç‡§ó‡§†‡§®           ‚Üí ‡§∏‡§Ç‡§ó‡§†‡§®\n",
      "  ‡§ï‡§∞‡•ç‡§Æ‡§Ø‡•ã‡§ó‡•Ä        ‚Üí ‡§ï‡§∞‡•ç‡§Æ + ‡§Ø‡•ã‡§ó‡•Ä\n",
      "  ‡§∏‡§Ç‡§µ‡•à‡§ß‡§æ‡§®‡§ø‡§ï       ‚Üí ‡§∏‡§Ç‡§µ‡•à‡§ß‡§æ‡§®‡§ø‡§ï\n",
      "  ‡§Ö‡§ß‡§ø‡§ó‡•ç‡§∞‡§π‡§£        ‚Üí ‡§Ö‡§ß‡§ø‡§ó‡•ç‡§∞‡§π‡§£\n",
      "  ‡§Ö‡§≠‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§£       ‚Üí ‡§Ö + ‡§≠‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§£\n",
      "  ‡§Ö‡§®‡•Å‡§≠‡•Ç‡§§‡§ø‡§ï‡§∞‡§£      ‚Üí ‡§Ö‡§®‡•Å‡§≠‡•Ç‡§§ + ‡§ø‡§ï‡§∞‡§£\n"
     ]
    }
   ],
   "source": [
    "from akshar import get_hindi_segmenter, segment_hindi\n",
    "\n",
    "segmenter = get_hindi_segmenter()\n",
    "\n",
    "if segmenter.is_model_loaded():\n",
    "    print(\"‚úì Hindi Morfessor model loaded!\\n\")\n",
    "    print(\"Morphological Segmentation:\\n\")\n",
    "    \n",
    "    # hindi_words = [\"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\", \"‡§Ö‡§ö‡•ç‡§õ‡§æ\", \"‡§∏‡§Æ‡§ù‡§®‡§æ\", \"‡§≤‡§°‡§º‡§ï‡•Ä\", \"‡§ï‡§∞‡§®‡§æ\"]\n",
    "    hindi_words = [\n",
    "        \"‡§ñ‡§æ‡§®‡§æ\", \"‡§™‡•Ä‡§®‡§æ\", \"‡§¶‡•á‡§ñ‡§®‡§æ\", \"‡§ö‡§≤‡§®‡§æ\", \"‡§∏‡•ã‡§®‡§æ\",\n",
    "        \"‡§∏‡•Å‡§®‡§®‡§æ\", \"‡§ñ‡•á‡§≤‡§®‡§æ\", \"‡§≤‡§ø‡§ñ‡§®‡§æ\", \"‡§™‡§¢‡§º‡§®‡§æ\", \"‡§∏‡•Ä‡§ñ‡§®‡§æ\",\n",
    "        \"‡§ó‡§æ‡§®‡§æ\", \"‡§π‡§Å‡§∏‡§®‡§æ\", \"‡§∞‡•ã‡§®‡§æ\", \"‡§∏‡•ã‡§ö‡§®‡§æ\", \"‡§¨‡•ã‡§≤‡§®‡§æ\",\n",
    "\n",
    "        \"‡§™‡§¢‡§º‡§æ‡§à\", \"‡§≤‡§°‡§º‡§æ‡§à\", \"‡§Æ‡§ø‡§ü‡•ç‡§ü‡•Ä\", \"‡§ú‡§≤‡•ç‡§¶‡•Ä\", \"‡§≠‡•Ç‡§ñ\",\n",
    "        \"‡§≠‡§Ø\", \"‡§ï‡§æ‡§¨‡•Ç\", \"‡§ï‡§π‡§æ‡§®‡•Ä\", \"‡§ñ‡•Å‡§∂‡•Ä\", \"‡§â‡§¶‡§æ‡§∏‡•Ä\",\n",
    "        \"‡§Æ‡§ú‡§¶‡•Ç‡§∞\", \"‡§Æ‡§π‡§ø‡§≤‡§æ\", \"‡§Ü‡§¶‡§Æ‡•Ä\", \"‡§ï‡§¨‡•Ç‡§§‡§∞\", \"‡§§‡§ø‡§§‡§≤‡•Ä\",\n",
    "\n",
    "        \"‡§ö‡•Å‡§®‡•å‡§§‡•Ä\", \"‡§â‡§™‡§Ø‡•ã‡§ó\", \"‡§Ö‡§®‡•Å‡§≠‡§µ\", \"‡§ö‡§ø‡§ï‡§ø‡§§‡•ç‡§∏‡§æ\", \"‡§®‡§æ‡§ó‡§∞‡§ø‡§ï\",\n",
    "        \"‡§Æ‡•Ç‡§≤‡•ç‡§Ø‡§æ‡§Ç‡§ï‡§®\", \"‡§Ü‡§µ‡§∂‡•ç‡§Ø‡§ï‡§§‡§æ\", \"‡§Ö‡§ú‡•ç‡§û‡§æ‡§®‡§§‡§æ\", \"‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ\", \"‡§™‡§∞‡§ø‡§≠‡§æ‡§∑‡§æ\",\n",
    "        \"‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø\", \"‡§µ‡§ø‡§ï‡§æ‡§∏\", \"‡§∏‡§Æ‡§∞‡•ç‡§™‡§£\", \"‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏\", \"‡§∏‡§Æ‡§∏‡•ç‡§Ø‡§æ\",\n",
    "\n",
    "        \"‡§â‡§§‡•ç‡§§‡§∞‡§¶‡§æ‡§Ø‡§ø‡§§‡•ç‡§µ\", \"‡§∏‡•ç‡§µ‡§§‡§Ç‡§§‡•ç‡§∞‡§§‡§æ\", \"‡§®‡§æ‡§ó‡§∞‡§ø‡§ï‡§§‡§æ\", \"‡§™‡•ç‡§∞‡§¨‡§Ç‡§ß‡§ï\", \"‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§§‡§æ\",\n",
    "        \"‡§¶‡§æ‡§Ø‡§ø‡§§‡•ç‡§µ‡§π‡•Ä‡§®‡§§‡§æ\", \"‡§Ö‡§®‡•Å‡§∂‡§æ‡§∏‡§®‡§π‡•Ä‡§®‡§§‡§æ\", \"‡§µ‡•ç‡§Ø‡§µ‡§∏‡•ç‡§•‡§æ‡§™‡§®\", \"‡§®‡§ø‡§∞‡•ç‡§µ‡§æ‡§ö‡§®\", \"‡§∏‡§Ç‡§ó‡§†‡§®\",\n",
    "        \"‡§ï‡§∞‡•ç‡§Æ‡§Ø‡•ã‡§ó‡•Ä\", \"‡§∏‡§Ç‡§µ‡•à‡§ß‡§æ‡§®‡§ø‡§ï\", \"‡§Ö‡§ß‡§ø‡§ó‡•ç‡§∞‡§π‡§£\", \"‡§Ö‡§≠‡§ø‡§Ø‡§Ç‡§§‡•ç‡§∞‡§£\", \"‡§Ö‡§®‡•Å‡§≠‡•Ç‡§§‡§ø‡§ï‡§∞‡§£\"\n",
    "    ]\n",
    "\n",
    "    \n",
    "    for word in hindi_words:\n",
    "        morphemes = segmenter.segment_word(word)\n",
    "        print(f\"  {word:15} ‚Üí {' + '.join(morphemes)}\")\n",
    "else:\n",
    "    print(\"‚ö† Morfessor not installed\")\n",
    "    print(\"Install with: pip install morfessor\")\n",
    "    print(\"\\nNote: Morphological segmentation will fall back to basic mode\")\n",
    "    # 75% accuracy i think for startig its good i wil keep training better mdl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d1b7b",
   "metadata": {},
   "source": [
    "### Feature 20: Complete Feature Comparison\n",
    "\n",
    "akshar vs Basic Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f3beb",
   "metadata": {},
   "source": [
    "## 7. Composition Analysis\n",
    "\n",
    "Analyze text composition across different types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "105b23a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Composition Analysis:\n",
      "\n",
      "Type            akshars    Switches   Devanagari   Roman     \n",
      "=================================================================\n",
      "Pure Hindi      15         0          100.0%       0.0%      \n",
      "Pure English    26         0          0.0%         100.0%    \n",
      "Hinglish        23         2          40.0%        60.0%     \n",
      "Sanskrit        17         0          100.0%       0.0%      \n"
     ]
    }
   ],
   "source": [
    "from akshar.segment import analyze_text_composition\n",
    "\n",
    "test_texts = {\n",
    "    \"Pure Hindi\": \"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\",\n",
    "    \"Pure English\": \"today weather is very nice\",\n",
    "    \"Hinglish\": \"aaj ka ‡§Æ‡•å‡§∏‡§Æ ‡§¨‡§π‡•Å‡§§ nice hai\",\n",
    "    \"Sanskrit\": \"‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á ‡§ß‡§∞‡•ç‡§Æ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á ‡§∏‡§Æ‡§µ‡•á‡§§‡§æ ‡§Ø‡•Å‡§Ø‡•Å‡§§‡•ç‡§∏‡§µ‡§É\",\n",
    "}\n",
    "\n",
    "print(\"Text Composition Analysis:\\n\")\n",
    "print(f\"{'Type':<15} {'akshars':<10} {'Switches':<10} {'Devanagari':<12} {'Roman':<10}\")\n",
    "print(\"=\"*65)\n",
    "\n",
    "for name, text in test_texts.items():\n",
    "    comp = analyze_text_composition(text)\n",
    "    print(f\"{name:<15} {comp['akshar_count']:<10} {comp['script_switches']:<10} \"\n",
    "          f\"{comp['devanagari_ratio']:<12.1%} {comp['roman_ratio']:<10.1%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc897b",
   "metadata": {},
   "source": [
    "## 8. Tokenization with Metadata\n",
    "\n",
    "Get tokens plus all analysis in one call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "67d4b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization with Metadata:\n",
      "\n",
      "Original:       hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç\n",
      "Normalized:     hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç\n",
      "Token count:    12\n",
      "akshar count:  12\n",
      "Script switches: 1\n",
      "Tokens:         h | e | l | l | o |   | ‡§® | ‡§Æ | ‡§∏‡•ç‡§§‡•á |   | ‡§¶‡•ã | ‡§∏‡•ç‡§§‡•ã‡§Ç\n"
     ]
    }
   ],
   "source": [
    "text = \"hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•ã‡§∏‡•ç‡§§‡•ã‡§Ç\"\n",
    "result = tokenizer.tokenize(text, return_metadata=True)\n",
    "\n",
    "print(\"Tokenization with Metadata:\\n\")\n",
    "print(f\"Original:       {result['original_text']}\")\n",
    "print(f\"Normalized:     {result['normalized_text']}\")\n",
    "print(f\"Token count:    {result['token_count']}\")\n",
    "print(f\"akshar count:  {result['akshar_count']}\")\n",
    "print(f\"Script switches: {result['script_switches']}\")\n",
    "print(f\"Tokens:         {' | '.join(result['tokens'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20bde3",
   "metadata": {},
   "source": [
    "## 9. Script Detection\n",
    "\n",
    "Identify which script each character belongs to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9841ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character Script Detection:\n",
      "\n",
      "'‡§®' ‚Üí devanagari\n",
      "'a' ‚Üí roman\n",
      "'‡§ï' ‚Üí devanagari\n",
      "'h' ‚Üí roman\n",
      "'5' ‚Üí digit\n",
      "'.' ‚Üí punct\n",
      "'‡§Æ' ‚Üí devanagari\n",
      "'Z' ‚Üí roman\n"
     ]
    }
   ],
   "source": [
    "from akshar.segment import identify_script\n",
    "\n",
    "test_chars = ['‡§®', 'a', '‡§ï', 'h', '5', '.', '‡§Æ', 'Z']\n",
    "\n",
    "print(\"Character Script Detection:\\n\")\n",
    "for char in test_chars:\n",
    "    script = identify_script(char)\n",
    "    print(f\"'{char}' ‚Üí {script}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4aaf5",
   "metadata": {},
   "source": [
    "## 10. Visual Comparison\n",
    "\n",
    "Compare how different texts tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a38b71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization Comparison:\n",
      "\n",
      "             Text  akshars  Tokens  Switches Dev% Rom%\n",
      "           ‡§®‡§Æ‡§∏‡•ç‡§§‡•á        3       3         0 100%   0%\n",
      "          namaste        7       7         0   0% 100%\n",
      "     hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á        9       9         1  50%  50%\n",
      "          ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞        2       2         0 100%   0%\n",
      "yaar kya haal hai       17      17         0   0% 100%\n",
      "        ‡§Ü‡§ú ‡§ï‡§æ day        8       8         1  67%  33%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_texts = [\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\",\n",
    "    \"namaste\", \n",
    "    \"hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á\",\n",
    "    \"‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞\",\n",
    "    \"yaar kya haal hai\",\n",
    "    \"‡§Ü‡§ú ‡§ï‡§æ day\",\n",
    "]\n",
    "\n",
    "data = []\n",
    "for text in comparison_texts:\n",
    "    analysis = tokenizer.explain(text)\n",
    "    data.append({\n",
    "        'Text': text,\n",
    "        'akshars': analysis['stats']['akshar_count'],\n",
    "        'Tokens': len(analysis['tokens']),\n",
    "        'Switches': analysis['stats']['script_switches'],\n",
    "        'Dev%': f\"{analysis['stats']['devanagari_ratio']:.0%}\",\n",
    "        'Rom%': f\"{analysis['stats']['roman_ratio']:.0%}\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"\\nTokenization Comparison:\\n\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364f290",
   "metadata": {},
   "source": [
    "## 11. Real-World Examples\n",
    "\n",
    "Social media style Hinglish text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e192922",
   "metadata": {},
   "source": [
    "## 21. üÜï Phonetic Analysis\n",
    "\n",
    "Analyze phonetic properties: vowels, consonants, aspiration, nasalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9603c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phonetic Properties of Characters:\n",
      "\n",
      "‡§ï:\n",
      "  Type: Consonant\n",
      "  Aspirated: False\n",
      "  Voiced: False\n",
      "  Nasal: False\n",
      "  Place: velar\n",
      "\n",
      "‡§ñ:\n",
      "  Type: Consonant\n",
      "  Aspirated: True\n",
      "  Voiced: False\n",
      "  Nasal: False\n",
      "  Place: velar\n",
      "\n",
      "‡§®:\n",
      "  Type: Consonant\n",
      "  Aspirated: False\n",
      "  Voiced: True\n",
      "  Nasal: True\n",
      "  Place: dental\n",
      "\n",
      "‡§Æ:\n",
      "  Type: Consonant\n",
      "  Aspirated: False\n",
      "  Voiced: True\n",
      "  Nasal: True\n",
      "  Place: labial\n",
      "\n",
      "‡§Ö:\n",
      "  Type: Vowel\n",
      "\n",
      "‡§Ü:\n",
      "  Type: Vowel\n",
      "\n",
      "==================================================\n",
      "\n",
      "Word Analysis:\n",
      "\n",
      "‡§®‡§Æ‡§∏‡•ç‡§§‡•á     ‚Üí Vowels: 1, Consonants: 4, Nasals: 2, Aspirated: 1\n",
      "‡§≠‡§æ‡§∞‡§§       ‚Üí Vowels: 1, Consonants: 3, Nasals: 0, Aspirated: 1\n",
      "‡§ñ‡§º‡•Å‡§∂‡•Ä      ‚Üí Vowels: 2, Consonants: 2, Nasals: 0, Aspirated: 2\n"
     ]
    }
   ],
   "source": [
    "from akshar import get_phonetic_analyzer\n",
    "\n",
    "analyzer = get_phonetic_analyzer()\n",
    "\n",
    "print(\"Phonetic Properties of Characters:\\n\")\n",
    "\n",
    "# test various characters\n",
    "chars = ['‡§ï', '‡§ñ', '‡§®', '‡§Æ', '‡§Ö', '‡§Ü']\n",
    "\n",
    "for char in chars:\n",
    "    props = analyzer.get_properties(char)\n",
    "    if props:\n",
    "        print(f\"{char}:\")\n",
    "        print(f\"  Type: {'Vowel' if props['is_vowel'] else 'Consonant'}\")\n",
    "        if props['is_consonant']:\n",
    "            print(f\"  Aspirated: {props['aspirated']}\")\n",
    "            print(f\"  Voiced: {props['voiced']}\")\n",
    "            print(f\"  Nasal: {props['nasal']}\")\n",
    "            place = analyzer.get_place_of_articulation(char)\n",
    "            if place:\n",
    "                print(f\"  Place: {place}\")\n",
    "        print()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"\\nWord Analysis:\\n\")\n",
    "\n",
    "words = [\"‡§®‡§Æ‡§∏‡•ç‡§§‡•á\", \"‡§≠‡§æ‡§∞‡§§\", \"‡§ñ‡§º‡•Å‡§∂‡•Ä\"]\n",
    "\n",
    "for word in words:\n",
    "    analysis = analyzer.analyze_word(word)\n",
    "    print(f\"{word:10} ‚Üí Vowels: {analysis['vowels']}, \"\n",
    "          f\"Consonants: {analysis['consonants']}, \"\n",
    "          f\"Nasals: {analysis['nasals']}, \"\n",
    "          f\"Aspirated: {analysis['aspirated']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399f7cfd",
   "metadata": {},
   "source": [
    "## 22. üÜï Multi-Script Detection\n",
    "\n",
    "Detect multiple Indic scripts (not just Devanagari)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "635b7e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§≠‡§æ‡§∞‡§§\n",
      "Scripts: {'devanagari': 10}\n",
      "\n",
      "Text: Hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á World\n",
      "Total chars: 18\n",
      "Indic chars: 6\n",
      "Scripts: {'devanagari': 6}\n",
      "Multilingual: False\n",
      "\n",
      "==================================================\n",
      "\n",
      "Script Identification Examples:\n",
      "\n",
      "‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç                      ‚Üí Scripts: ['devanagari']\n",
      "‡§π‡§ø‡§®‡•ç‡§¶‡•Ä                         ‚Üí Scripts: ['devanagari']\n",
      "aaj ka din                     ‚Üí Scripts: ['roman']\n",
      "‡§Æ‡•à‡§Ç California ‡§ú‡§æ‡§§‡§æ ‡§π‡•Ç‡§Ç        ‚Üí Scripts: ['devanagari']\n"
     ]
    }
   ],
   "source": [
    "from akshar import identify_scripts, analyze_script\n",
    "\n",
    "# pure devanagari\n",
    "text1 = \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§≠‡§æ‡§∞‡§§\"\n",
    "scripts1 = identify_scripts(text1)\n",
    "print(f\"Text: {text1}\")\n",
    "print(f\"Scripts: {scripts1}\\n\")\n",
    "\n",
    "# mixed content\n",
    "text2 = \"Hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á World\"\n",
    "analysis2 = analyze_script(text2)\n",
    "print(f\"Text: {text2}\")\n",
    "print(f\"Total chars: {analysis2['total_chars']}\")\n",
    "print(f\"Indic chars: {analysis2['indic_chars']}\")\n",
    "print(f\"Scripts: {analysis2['scripts']}\")\n",
    "print(f\"Multilingual: {analysis2['is_multilingual']}\\n\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"\\nScript Identification Examples:\\n\")\n",
    "\n",
    "test_texts = [\n",
    "    \"‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§Æ‡•ç\",           # Sanskrit/Devanagari\n",
    "    \"‡§π‡§ø‡§®‡•ç‡§¶‡•Ä\",              # Hindi\n",
    "    \"aaj ka din\",         # Roman\n",
    "    \"‡§Æ‡•à‡§Ç California ‡§ú‡§æ‡§§‡§æ ‡§π‡•Ç‡§Ç\",  # Mixed\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    scripts = identify_scripts(text)\n",
    "    analysis = analyze_script(text)\n",
    "    print(f\"{text:30} ‚Üí Scripts: {list(scripts.keys()) if scripts else ['roman']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3870adec",
   "metadata": {},
   "source": [
    "## 23. üÜï Combined Intelligence Pipeline\n",
    "\n",
    "Use all features together for complete analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1dcbf2",
   "metadata": {},
   "source": [
    "## 24. üÜï Character Deep Dive\n",
    "\n",
    "Examine individual character properties in detail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8dad9787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Character Analysis:\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Character: ‡§ï\n",
      "Unicode: U+0915\n",
      "Name: DEVANAGARI LETTER KA\n",
      "ITRANS: ka\n",
      "Type: Consonant\n",
      "Articulation: velar\n",
      "Aspirated: False\n",
      "Voiced: False\n",
      "Nasal: False\n",
      "\n",
      "Character: ‡§ñ\n",
      "Unicode: U+0916\n",
      "Name: DEVANAGARI LETTER KHA\n",
      "ITRANS: kha\n",
      "Type: Consonant\n",
      "Articulation: velar\n",
      "Aspirated: True\n",
      "Voiced: False\n",
      "Nasal: False\n",
      "\n",
      "Character: ‡§®\n",
      "Unicode: U+0928\n",
      "Name: DEVANAGARI LETTER NA\n",
      "ITRANS: na\n",
      "Type: Consonant\n",
      "Articulation: dental\n",
      "Aspirated: False\n",
      "Voiced: True\n",
      "Nasal: True\n",
      "\n",
      "Character: ‡§Æ\n",
      "Unicode: U+092E\n",
      "Name: DEVANAGARI LETTER MA\n",
      "ITRANS: ma\n",
      "Type: Consonant\n",
      "Articulation: labial\n",
      "Aspirated: False\n",
      "Voiced: True\n",
      "Nasal: True\n",
      "\n",
      "Character: ‡§Ö\n",
      "Unicode: U+0905\n",
      "Name: DEVANAGARI LETTER A\n",
      "ITRANS: a\n",
      "Type: Vowel\n",
      "\n",
      "Character: ‡§Ü\n",
      "Unicode: U+0906\n",
      "Name: DEVANAGARI LETTER AA\n",
      "ITRANS: A\n",
      "Type: Vowel\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Note: Conjuncts like ‡§ö‡•ç and ‡§ú‡•ç‡§û are multiple unicode codepoints,\n",
      "      so they need special handling for character-level analysis.\n"
     ]
    }
   ],
   "source": [
    "from akshar.script_utils import ScriptAnalyzer\n",
    "\n",
    "script_analyzer = ScriptAnalyzer()\n",
    "phonetic = get_phonetic_analyzer()\n",
    "\n",
    "# interesting characters\n",
    "chars_to_examine = ['‡§ï', '‡§ñ', '‡§®', '‡§Æ', '‡§Ö', '‡§Ü']\n",
    "\n",
    "print(\"Deep Character Analysis:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for char in chars_to_examine:\n",
    "    print(f\"\\nCharacter: {char}\")\n",
    "    print(f\"Unicode: U+{ord(char):04X}\")\n",
    "    \n",
    "    # unicode name\n",
    "    name = script_analyzer.get_character_name(char)\n",
    "    print(f\"Name: {name}\")\n",
    "    \n",
    "    # phonetic props\n",
    "    props = phonetic.get_properties(char)\n",
    "    if props:\n",
    "        print(f\"ITRANS: {props['itrans']}\")\n",
    "        if props['is_vowel']:\n",
    "            print(f\"Type: Vowel\")\n",
    "        elif props['is_consonant']:\n",
    "            print(f\"Type: Consonant\")\n",
    "            place = phonetic.get_place_of_articulation(char)\n",
    "            if place:\n",
    "                print(f\"Articulation: {place}\")\n",
    "            print(f\"Aspirated: {props['aspirated']}\")\n",
    "            print(f\"Voiced: {props['voiced']}\")\n",
    "            print(f\"Nasal: {props['nasal']}\")\n",
    "        \n",
    "        if props['halanta']:\n",
    "            print(\"‚ö† Halanta (virama)\")\n",
    "        if props['anusvara']:\n",
    "            print(\"‚ö† Anusvara\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "\n",
    "print(\"\\nNote: Conjuncts like ‡§ö‡•ç and ‡§ú‡•ç‡§û are multiple unicode codepoints,\")\n",
    "print(\"      so they need special handling for character-level analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c85e21",
   "metadata": {},
   "source": [
    "## 25. üÜï Linguistic Feature Extraction\n",
    "\n",
    "Extract all linguistic features for ML/NLP tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e033c7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Feature Extraction:\n",
      "\n",
      "======================================================================\n",
      "             text  length  tokens  akshars  devanagari_ratio  roman_ratio  script_switches  vowel_count  consonant_count\n",
      "      ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§≠‡§æ‡§∞‡§§      11       7        7          1.000000     0.000000                1            2                7\n",
      "      hello world      11      11       11          0.000000     1.000000                1            0                0\n",
      "aaj ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à      17      12       12          0.764706     0.235294                2            4                6\n",
      "   ‡§Ø‡§æ‡§∞ kya ‡§¨‡§æ‡§§ ‡§π‡•à      14      11       11          0.714286     0.285714                3            3                5\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üí° These features can be used for:\n",
      "   ‚Ä¢ Text classification\n",
      "   ‚Ä¢ Language identification\n",
      "   ‚Ä¢ Code-mixing detection\n",
      "   ‚Ä¢ Sentiment analysis\n",
      "   ‚Ä¢ ML model training\n"
     ]
    }
   ],
   "source": [
    "from akshar import (\n",
    "    aksharTokenizer,\n",
    "    detect_code_switches,\n",
    "    normalize_text,\n",
    "    analyze_phonetics,\n",
    "    analyze_script,\n",
    "    segment_akshars\n",
    ")\n",
    "\n",
    "def extract_all_features(text):\n",
    "    \"\"\"\n",
    "    Extract complete linguistic feature vector.\n",
    "    Useful for ML models, text classification, etc.\n",
    "    \"\"\"\n",
    "    tokenizer = aksharTokenizer()\n",
    "    \n",
    "    # get all analyses\n",
    "    token_analysis = tokenizer.explain(text)\n",
    "    script_analysis = analyze_script(text)\n",
    "    phonetic_analysis = analyze_phonetics(text)\n",
    "    akshars = segment_akshars(text)\n",
    "    switches = detect_code_switches(text)\n",
    "    \n",
    "    features = {\n",
    "        # basic\n",
    "        'length': len(text),\n",
    "        'tokens': len(token_analysis['tokens']),\n",
    "        'akshars': len(akshars),\n",
    "        \n",
    "        # script composition\n",
    "        'devanagari_ratio': token_analysis['stats']['devanagari_ratio'],\n",
    "        'roman_ratio': token_analysis['stats']['roman_ratio'],\n",
    "        'script_switches': len(switches),\n",
    "        'is_multilingual': script_analysis['is_multilingual'],\n",
    "        \n",
    "        # phonetic\n",
    "        'vowel_count': phonetic_analysis['vowels'],\n",
    "        'consonant_count': phonetic_analysis['consonants'],\n",
    "        'nasal_count': phonetic_analysis['nasals'],\n",
    "        'aspirated_count': phonetic_analysis['aspirated'],\n",
    "        \n",
    "        # ratios\n",
    "        'vowel_consonant_ratio': (\n",
    "            phonetic_analysis['vowels'] / max(phonetic_analysis['consonants'], 1)\n",
    "        ),\n",
    "        'akshar_per_token': len(akshars) / max(len(token_analysis['tokens']), 1),\n",
    "    }\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "# test it\n",
    "test_sentences = [\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§≠‡§æ‡§∞‡§§\",\n",
    "    \"hello world\",\n",
    "    \"aaj ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\",\n",
    "    \"‡§Ø‡§æ‡§∞ kya ‡§¨‡§æ‡§§ ‡§π‡•à\"\n",
    "]\n",
    "\n",
    "print(\"Complete Feature Extraction:\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "features_list = []\n",
    "for sent in test_sentences:\n",
    "    features = extract_all_features(sent)\n",
    "    features['text'] = sent\n",
    "    features_list.append(features)\n",
    "\n",
    "df = pd.DataFrame(features_list)\n",
    "\n",
    "# reorder columns\n",
    "cols = ['text', 'length', 'tokens', 'akshars', 'devanagari_ratio', \n",
    "        'roman_ratio', 'script_switches', 'vowel_count', 'consonant_count']\n",
    "print(df[cols].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nüí° These features can be used for:\")\n",
    "print(\"   ‚Ä¢ Text classification\")\n",
    "print(\"   ‚Ä¢ Language identification\")\n",
    "print(\"   ‚Ä¢ Code-mixing detection\")\n",
    "print(\"   ‚Ä¢ Sentiment analysis\")\n",
    "print(\"   ‚Ä¢ ML model training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c2c1fc1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social Media Hinglish Examples:\n",
      "\n",
      "Original:   yaaaar aaj ka mausam bohot achaaaa hai\n",
      "Normalized: yar aaj ka mausam bohot acha hai\n",
      "Tokens:     y | a | r |   | a | a | j |   | k | a |   | m | a | u | s...\n",
      "\n",
      "Original:   ‡§Æ‡•Å‡§ù‡•á traveling ‡§ï‡§∞‡§®‡§æ ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à\n",
      "Normalized: ‡§Æ‡•Å‡§ù‡•á traveling ‡§ï‡§∞‡§®‡§æ ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à\n",
      "Tokens:     ‡§Æ‡•Å | ‡§ù‡•á |   | t | r | a | v | e | l | i | n | g |   | ‡§ï | ‡§∞...\n",
      "\n",
      "Original:   ‡§Ü‡§ú ‡§ï‡§æ day ‡§¨‡§π‡•Å‡§§ productive ‡§•‡§æ bro\n",
      "Normalized: ‡§Ü‡§ú ‡§ï‡§æ day ‡§¨‡§π‡•Å‡§§ productive ‡§•‡§æ bro\n",
      "Tokens:     ‡§Ü | ‡§ú |   | ‡§ï‡§æ |   | d | a | y |   | ‡§¨ | ‡§π‡•Å | ‡§§ |   | p | r...\n",
      "\n",
      "Original:   ‡§ö‡§≤‡•ã coffee ‡§™‡•Ä‡§§‡•á ‡§π‡•à‡§Ç\n",
      "Normalized: ‡§ö‡§≤‡•ã coffee ‡§™‡•Ä‡§§‡•á ‡§π‡•à‡§Ç\n",
      "Tokens:     ‡§ö | ‡§≤‡•ã |   | c | o | f | f | e | e |   | ‡§™‡•Ä | ‡§§‡•á |   | ‡§π‡•à‡§Ç...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "social_media_texts = [\n",
    "    \"yaaaar aaj ka mausam bohot achaaaa hai\",\n",
    "    \"‡§Æ‡•Å‡§ù‡•á traveling ‡§ï‡§∞‡§®‡§æ ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à\",\n",
    "    \"‡§Ü‡§ú ‡§ï‡§æ day ‡§¨‡§π‡•Å‡§§ productive ‡§•‡§æ bro\",\n",
    "    \"‡§ö‡§≤‡•ã coffee ‡§™‡•Ä‡§§‡•á ‡§π‡•à‡§Ç\",\n",
    "]\n",
    "\n",
    "print(\"Social Media Hinglish Examples:\\n\")\n",
    "for text in social_media_texts:\n",
    "    analysis = tokenizer.explain(text)\n",
    "    print(f\"Original:   {text}\")\n",
    "    print(f\"Normalized: {analysis['normalized']}\")\n",
    "    print(f\"Tokens:     {' | '.join(analysis['tokens'][:15])}...\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f889edf3",
   "metadata": {},
   "source": [
    "## 12. Sanskrit Text Processing\n",
    "\n",
    "Complex conjuncts and classical texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bd6f22b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanskrit Text Processing:\n",
      "\n",
      "Text: ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á ‡§ß‡§∞‡•ç‡§Æ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á ‡§∏‡§Æ‡§µ‡•á‡§§‡§æ ‡§Ø‡•Å‡§Ø‡•Å‡§§‡•ç‡§∏‡§µ‡§É\n",
      "akshars: 17, Tokens: 17\n",
      "Conjuncts preserved: ‡§ï‡•ç‡§∑‡•á, ‡§§‡•ç‡§∞‡•á, ‡§∞‡•ç‡§Æ, ‡§ï‡•ç‡§∑‡•á, ‡§§‡•ç‡§∞‡•á\n",
      "\n",
      "Text: ‡§ß‡§∞‡•ç‡§Æ ‡§ï‡•Ä ‡§ú‡§Ø ‡§π‡•ã\n",
      "akshars: 9, Tokens: 9\n",
      "Conjuncts preserved: ‡§∞‡•ç‡§Æ, ‡§ï‡•Ä, ‡§π‡•ã\n",
      "\n",
      "Text: ‡§ú‡•ç‡§û‡§æ‡§® ‡§∏‡•á ‡§π‡•Ä ‡§Æ‡•ã‡§ï‡•ç‡§∑ ‡§Æ‡§ø‡§≤‡§§‡§æ ‡§π‡•à\n",
      "akshars: 15, Tokens: 15\n",
      "Conjuncts preserved: ‡§ú‡•ç‡§û‡§æ, ‡§∏‡•á, ‡§π‡•Ä, ‡§Æ‡•ã, ‡§ï‡•ç‡§∑\n",
      "\n",
      "Text: ‡§µ‡•á‡§¶‡§æ‡§É ‡§ú‡•ç‡§û‡§æ‡§®‡§∏‡•ç‡§Ø ‡§∏‡•ç‡§∞‡•ã‡§§‡§É\n",
      "akshars: 9, Tokens: 9\n",
      "Conjuncts preserved: ‡§µ‡•á, ‡§¶‡§æ‡§É, ‡§ú‡•ç‡§û‡§æ, ‡§∏‡•ç‡§Ø, ‡§∏‡•ç‡§∞‡•ã\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sanskrit_texts = [\n",
    "    \"‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á ‡§ß‡§∞‡•ç‡§Æ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•á ‡§∏‡§Æ‡§µ‡•á‡§§‡§æ ‡§Ø‡•Å‡§Ø‡•Å‡§§‡•ç‡§∏‡§µ‡§É\",\n",
    "    \"‡§ß‡§∞‡•ç‡§Æ ‡§ï‡•Ä ‡§ú‡§Ø ‡§π‡•ã\",\n",
    "    \"‡§ú‡•ç‡§û‡§æ‡§® ‡§∏‡•á ‡§π‡•Ä ‡§Æ‡•ã‡§ï‡•ç‡§∑ ‡§Æ‡§ø‡§≤‡§§‡§æ ‡§π‡•à\",\n",
    "    \"‡§µ‡•á‡§¶‡§æ‡§É ‡§ú‡•ç‡§û‡§æ‡§®‡§∏‡•ç‡§Ø ‡§∏‡•ç‡§∞‡•ã‡§§‡§É\",\n",
    "]\n",
    "\n",
    "print(\"Sanskrit Text Processing:\\n\")\n",
    "for text in sanskrit_texts:\n",
    "    akshars = segment_akshars(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"akshars: {len(akshars)}, Tokens: {len(tokens)}\")\n",
    "    \n",
    "    conjuncts = [a for a in akshars if len(a) > 1 and a.strip()]\n",
    "    if conjuncts:\n",
    "        print(f\"Conjuncts preserved: {', '.join(conjuncts[:5])}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f9ffe",
   "metadata": {},
   "source": [
    "## 13. Visualization Helpers\n",
    "\n",
    "Color-coded output for terminal display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cf1d0074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization Formats:\n",
      "\n",
      "Token Boundaries:\n",
      "a | a | j |   | ‡§Æ‡•å | ‡§∏ | ‡§Æ |   | ‡§Ö | ‡§ö‡•ç‡§õ‡§æ |   | ‡§π‡•à\n",
      "\n",
      "akshar Boundaries:\n",
      "[a] [a] [j] [ ] [‡§Æ‡•å] [‡§∏] [‡§Æ] [ ] [‡§Ö] [‡§ö‡•ç‡§õ‡§æ] [ ] [‡§π‡•à]\n"
     ]
    }
   ],
   "source": [
    "from akshar.viz import format_token_boundaries, format_akshar_boundaries\n",
    "\n",
    "text = \"aaj ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "akshars = segment_akshars(text)\n",
    "\n",
    "print(\"Visualization Formats:\\n\")\n",
    "print(\"Token Boundaries:\")\n",
    "print(format_token_boundaries(text, tokens))\n",
    "print(\"\\nakshar Boundaries:\")\n",
    "print(format_akshar_boundaries(akshars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e3a0e5",
   "metadata": {},
   "source": [
    "## 14. Batch Processing\n",
    "\n",
    "Process multiple texts efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "803da4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Processing Results:\n",
      "\n",
      "‡§≠‡§æ‡§∞‡§§ ‡§è‡§ï ‡§Æ‡§π‡§æ‡§® ‡§¶‡•á‡§∂ ‡§π‡•à                      | Tokens: 15  | Dev: 100%\n",
      "India is a great country                 | Tokens: 24  | Dev: 0%\n",
      "‡§Æ‡•à‡§Ç India ‡§∏‡•á ‡§π‡•Ç‡§Ç                         | Tokens: 11  | Dev: 62%\n",
      "‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ interesting ‡§π‡•à                   | Tokens: 20  | Dev: 45%\n",
      "sanskritam ‡§¶‡•á‡§µ‡§≠‡§æ‡§∑‡§æ ‡§Ö‡§∏‡•ç‡§§‡§ø                 | Tokens: 18  | Dev: 54%\n"
     ]
    }
   ],
   "source": [
    "batch_texts = [\n",
    "    \"‡§≠‡§æ‡§∞‡§§ ‡§è‡§ï ‡§Æ‡§π‡§æ‡§® ‡§¶‡•á‡§∂ ‡§π‡•à\",\n",
    "    \"India is a great country\",\n",
    "    \"‡§Æ‡•à‡§Ç India ‡§∏‡•á ‡§π‡•Ç‡§Ç\",\n",
    "    \"‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ interesting ‡§π‡•à\",\n",
    "    \"sanskritam ‡§¶‡•á‡§µ‡§≠‡§æ‡§∑‡§æ ‡§Ö‡§∏‡•ç‡§§‡§ø\",\n",
    "]\n",
    "\n",
    "print(\"Batch Processing Results:\\n\")\n",
    "results = []\n",
    "for text in batch_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    analysis = tokenizer.explain(text)\n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'tokens': len(tokens),\n",
    "        'dev_ratio': analysis['stats']['devanagari_ratio']\n",
    "    })\n",
    "\n",
    "for r in results:\n",
    "    print(f\"{r['text']:<40} | Tokens: {r['tokens']:<3} | Dev: {r['dev_ratio']:.0%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980605a0",
   "metadata": {},
   "source": [
    "## 15. Summary Statistics\n",
    "\n",
    "Get overall statistics from the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5c98098e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Configuration:\n",
      "\n",
      "Model loaded: False\n",
      "Model type: akshar\n",
      "Normalize Roman: True\n",
      "Clean Hinglish: True\n",
      "Vocab size: 0\n",
      "\n",
      "============================================================\n",
      "\n",
      "akshar Features Demonstrated:\n",
      "  ‚úì Basic tokenization\n",
      "  ‚úì akshar segmentation (conjunct preservation)\n",
      "  ‚úì Code-switch detection\n",
      "  ‚úì Text normalization\n",
      "  ‚úì Hinglish handling\n",
      "  ‚úì Phonetic signatures\n",
      "  ‚úì Detailed analysis\n",
      "  ‚úì Composition analysis\n",
      "  ‚úì Script detection\n",
      "  ‚úì Metadata extraction\n",
      "  ‚úì Sanskrit processing\n",
      "  ‚úì Batch processing\n",
      "  ‚úì Visualization helpers\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenizer Configuration:\\n\")\n",
    "print(f\"Model loaded: {tokenizer.model is not None}\")\n",
    "print(f\"Model type: {tokenizer.model_type}\")\n",
    "print(f\"Normalize Roman: {tokenizer.normalize_roman}\")\n",
    "print(f\"Clean Hinglish: {tokenizer.clean_hinglish}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nakshar Features Demonstrated:\")\n",
    "print(\"  ‚úì Basic tokenization\")\n",
    "print(\"  ‚úì akshar segmentation (conjunct preservation)\")\n",
    "print(\"  ‚úì Code-switch detection\")\n",
    "print(\"  ‚úì Text normalization\")\n",
    "print(\"  ‚úì Hinglish handling\")\n",
    "print(\"  ‚úì Phonetic signatures\")\n",
    "print(\"  ‚úì Detailed analysis\")\n",
    "print(\"  ‚úì Composition analysis\")\n",
    "print(\"  ‚úì Script detection\")\n",
    "print(\"  ‚úì Metadata extraction\")\n",
    "print(\"  ‚úì Sanskrit processing\")\n",
    "print(\"  ‚úì Batch processing\")\n",
    "print(\"  ‚úì Visualization helpers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0b2a68cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§™‡•ç‡§∞', '‡§ï‡§æ', '‡§∂']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import akshara_level_tokenization\n",
    "akshara_level_tokenization(\"‡§™‡•ç‡§∞‡§ï‡§æ‡§∂\")  # ['‡§™‡•ç‡§∞', '‡§ï‡§æ', '‡§∂'] - halant-aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a92f95be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': '‡§¶‡•á‡§µ‡•ã‡§Ω‡§™‡§ø', 'boundaries': [], 'marked': '‡§¶‡•á‡§µ‡•ã‡§Ω‡§™‡§ø'}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import sandhi_aware_tokenization\n",
    "sandhi_aware_tokenization(\"‡§¶‡•á‡§µ‡•ã‡§Ω‡§™‡§ø\")  # marks boundaries, preserves original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c241118e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '‡§ï‡§Æ‡§≤', 'deletion_annotations': [('‡§ï‡§Æ‡§≤', [])]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import schwa_deletion_modeling\n",
    "schwa_deletion_modeling(\"‡§ï‡§Æ‡§≤\")  # annotates deletion positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59bcb76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§∂', '‡§ï‡•ç‡§§‡§ø']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import preserve_conjuncts\n",
    "preserve_conjuncts(\"‡§∂‡§ï‡•ç‡§§‡§ø\")  # ['‡§∂', '‡§ï‡•ç‡§§‡§ø'] - preserves conjuncts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "622da3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': '‡§∏‡§Ç‡§ï‡§∞', 'resolved': '‡§∏‡§ô‡§ï‡§∞'}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import intelligent_anusvara_resolution\n",
    "intelligent_anusvara_resolution(\"‡§∏‡§Ç‡§ï‡§∞\")  # {'original': '...', 'resolved': '‡§∏‡§ô‡•ç‡§ï‡§∞'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0ed3b337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§π‡§Å‡§∏']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import handle_chandrabindu\n",
    "handle_chandrabindu(\"‡§π‡§Å‡§∏\")  # ['‡§π‡§Å‡§∏'] - chandrabindu stays with vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ed03c3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§ï‡§º‡§¶', '‡§Æ']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import preserve_nukta\n",
    "preserve_nukta(\"‡§ï‡§º‡§¶‡§Æ\")  # preserves nukta characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "63ea6025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '‡§ï‡•ç‡§∑', 'halant_analysis': [(1, 'conjunct_formation', '‡§ï‡•ç‡§∑')]}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import recognize_virama_function\n",
    "recognize_virama_function(\"‡§ï‡•ç‡§∑\")  # analyzes halant context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "321a6f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'syllables': ['‡§ó‡§Ç', '‡§ó'],\n",
       " 'mora_counts': [2, 1],\n",
       " 'total_mora': 3,\n",
       " 'light_syllables': 1,\n",
       " 'heavy_syllables': 1}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import mora_aware_segmentation\n",
    "mora_aware_segmentation(\"‡§ó‡§Ç ‡§ó\")  # counts light/heavy syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4d054562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§∞‡§æ‡§Æ‡§É', '‡•§', '‡§ï‡•É‡§∑‡•ç‡§£‡§É', '‡••']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import sanskrit_punctuation_tokenization\n",
    "sanskrit_punctuation_tokenization(\"‡§∞‡§æ‡§Æ‡§É‡•§ ‡§ï‡•É‡§∑‡•ç‡§£‡§É‡••\")  # ['‡§∞‡§æ‡§Æ‡§É', '‡•§', '‡§ï‡•É‡§∑‡•ç‡§£‡§É', '‡••']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f6fdbb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§Ö‡•í‡§ó‡•ç‡§®‡§ø‡§É']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import preserve_svara_marks_feature\n",
    "preserve_svara_marks_feature(\"‡§Ö‡•í‡§ó‡•ç‡§®‡§ø‡§É\")  # preserves vedic accent marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "88fc8bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡•ß', '‡•®', '‡•©']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import devanagari_digit_tokenization\n",
    "devanagari_digit_tokenization(\"‡•ß‡•®‡•©\")  # ['‡•ß', '‡•®', '‡•©']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "11f4c185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§ï‡•ç\\u200d‡§Ø‡§æ'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import preserve_zwj\n",
    "preserve_zwj(\"‡§ï‡•ç‚Äç‡§Ø‡§æ\")  # preserves ZWJ/ZWNJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "74b4c8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§∂‡•ç‡§∞‡•Ä', '‡§Æ‡§Ç', '‡§ó', '‡§≤', '‡§Æ‡•ç']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import preserve_proper_names\n",
    "preserve_proper_names(\"‡§∂‡•ç‡§∞‡•Ä‡§Æ‡§Ç‡§ó‡§≤‡§Æ‡•ç\", names=['‡§∂‡•ç‡§∞‡•Ä'])  # preserves proper names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "095c7a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'surface_forms': ['‡§ó‡§ö‡•ç‡§õ‡§æ‡§Æ‡§ø'], 'lemma_hints': [('‡§ó‡§ö‡•ç‡§õ‡§æ‡§Æ‡§ø', '‡§ó‡§Æ‡•ç')]}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import provide_lemma_hints\n",
    "provide_lemma_hints(\"‡§ó‡§ö‡•ç‡§õ‡§æ‡§Æ‡§ø\", lemma_dict={'‡§ó‡§ö‡•ç‡§õ‡§æ‡§Æ‡§ø': '‡§ó‡§Æ‡•ç'})  # provides lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95904d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡§ã‡§∑‡§ø'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import preserve_orthographic_variants\n",
    "preserve_orthographic_variants(\"‡§ã‡§∑‡§ø\")  # preserves orthographic history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9c9a8514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_tokens': ['‡§ï‡•ç‡§∑'], 'transliterated': ['ka‡•ç·π£a'], 'scheme': 'iast'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import transliteration_tokenization\n",
    "transliteration_tokenization(\"‡§ï‡•ç‡§∑\", scheme='iast')  # IAST transliteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "69d4ad0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original': '‡§∞‡§æ‡§Æ‡§É ‡§∏‡§É', 'visarga_annotations': [], 'has_visarga': True}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import visarga_condition_tokenization\n",
    "visarga_condition_tokenization(\"‡§∞‡§æ‡§Æ‡§É ‡§∏‡§É\")  # analyzes visarga behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f883301c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§ì', '‡§Æ‡•ç']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import preserve_independent_vowels\n",
    "preserve_independent_vowels(\"‡§ì‡§Æ‡•ç\")  # ['‡§ì‡§Æ‡•ç'] - preserves independent vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1b95da8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‡§∂‡•Å', '‡§≠', ' ', '‡§∞‡§æ', '‡§§‡•ç‡§∞‡§ø', ' ', 'üåô']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from akshar import emoji_tokenization\n",
    "emoji_tokenization(\"‡§∂‡•Å‡§≠ ‡§∞‡§æ‡§§‡•ç‡§∞‡§ø üåô\")  # ['‡§∂‡•Å', '‡§≠', ' ', '‡§∞‡§æ', '‡§§‡•ç‡§∞‡§ø', ' ', 'üåô']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2848079e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d8ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a8bb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
